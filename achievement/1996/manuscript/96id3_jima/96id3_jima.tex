%%%修正履歴　　投稿     97.10.01    学会作成のstyle file を使用
%              修正     97.11.05    アルゴリズムの番号の振り間違えを修正
%
%  
%\documentstyle[epsbox,jsci8]{jima}
\documentstyle[epsbox,bmset]{jima}

%-------------------------------------------------------------------------
%\blm{*}の定義      added by T.Miyoshi 97/03/05
\newcommand{\blm}[1]{\mbox{\boldmath $#1$}}
%-------------------------------------------------------------------------

\etitle{Approximation of Multi-Dimensional Mapping by Bernstein Polinomial and Fuzzy ID3}
\eauthor{%
Hidetomo I{\sc chihashi},\ethanks{Osaka Prefecture University}%
\and% 
Tetsuya M{\sc iyoshi}\footnum{1}%
\and% 
Kazunori N{\sc agasaka}\footnum{1}%
\AND%
Tadashi S{\sc hirai}\footnum{1}
}



\eabstract{
Selection of the variables and nonlinear terms of polinomials for approximating nonlinear multi-dimensional mappings has many difficulties, since it is a problem of a kind of combinatorial optimization. The group method of data handling(GMDH) is an algorithm for the nonlinear model identification from collected regression-type data. But, it needs to culculate for many combinations of polinomials which are called the partial descriptions. 
The combinatorial algorithms are computationally intensive. This paper proposes a simple method of model identification using the fuzzy ID3 and Bernstein polinomial, in which the B-spline is regarded as the membership function of fuzzy set. 
From a practical view point, an algorithm with less computational cost for selecting the terms of polinomial is discussed as well as links to rule extraction metods from numerical data. Since the obtained polinomial is consist of the product of B-splines, by regarding B-splines as the menbership functions of fuzzy sets, it can be represented as the fuzzy if-then rule type knowledges and fuzzy decision trees. 
}
\ekeyword{Fuzzy ID3, Decision tree, Selection of variable}
\ereceived{199?}{???..}{??}
\eaccepted{199?}{???..}{??}

\title{ファジィＩＤ３を用いたBernstein多項式による多次元写像の近似法}
\author{%
市\hskip1zw橋\hskip1zw秀\hskip1zw友
\thanks{大阪府立大学（Osaka Prefecture University）}\,\,，
三\hskip1zw好\hskip1zw哲\hskip1zw也
\footnum{1}\,\,，
長\hskip1zw坂\hskip1zw一\hskip1zw徳
\footnum{1}\,\,，
白\hskip1zw井\hskip1zw正\hskip1zw志
\footnum{1}\,\,，
}
\abstract{%
　非線形な入出力関係や説明変数と応答変量との関係を多項式で近似しようとした時の変数の選択と必要な項の選択は一種の組み合わせ最適化問題であり，多変数を扱う問題では計算コスト上の困難さがある．変数組み合わせ計算法（ＧＭＤＨ）はそのための有効な手法であるが，基本的には非常に多くの組み合わせを計算する必要があり，簡単なモデル同定法とはいえない．本研究ではBernstein多項式による簡便なモデル同定法を取り上げる．B-スプラインをファジィ集合のメンバシップ関数とみなして，ファジィＩＤ３を適用することによって，多項式近似によるモデル同定が容易に可能であることを実験データを用いた数値例により示す．同定されたBernstein多項式は内部節点のない２次のB-スプラインの積の項からなっているので，それをメンバシップ関数と見なすことによってファジィルール形式の知識として，また木構造の決定木として表すことができる．
}
\keyword{ファジィID3，決定木，変数選択}
\received{199?年?月??日}
\accepted{199?年?月??日}
\VOL{??}
\NO{?}
\YEAR{199?}
\setcounter{page}{1}

\begin{document}
\maketitle

\section{はじめに}

エキスパートシステムを構築する際のボトルネックは，専門家からの知識獲得の困難さにあると言われている．このボトルネックを克服するためのアプローチにデータベースからの知識発見技術(Knowledge Discovery in Database, KDD)が注目されている\cite{maeda}．蓄積されたデータの集合から規則性や関係を知識として抽出するこのような方法はデータマイニング(Data Mining)とも呼ばれる．KDDシステムの代表例としてQuinlan\cite{quinlan,forsyth}によるＩＤ３(Iteractive Dichotomizer 3)がある\cite{maeda}．ＩＤ３では知識表現のための決定木の作成における属性の識別力の評価方法に情報理論における情報量（エントロピー）を採用している．ＩＤ３は簡単で強力な決定木作成法であり，新しい対象を分類するときのテスト回数最小化をその目的としている．
通常のＩＤ３では数値属性を取り扱うことができないため，属性値にファジィ集合を用いるファジィＩＤ３が提案されている．馬野ら\cite{馬野}は専門家の経験を反映したファジィ集合を用いて油中ガス分析診断に応用している．ファジィＩＤ３の決定木はファジィ推論のルールの集まりとみなすことができ，木の生成はファジィルールの抽出，もしくはファジィパーティションの決定に相当している．ファジィルールをデータから自動的に獲得する方法はファジィモデリングと呼ばれ，菅野・姜\cite{sugeno}による手法では，不偏性規範\cite{ivakhnenko} による前進法でのファジィ分割やコンプレックス法による最適化，後退消去法による線形式の係数同定が行われる．これらの手法は，対話型やスーパバイザー方式などのモデラーの直観や主観的判断を取り入れたファジィ手法\cite{中森}へと発展してきている．また，ファジィルールの自動獲得の手法の１つとして，データ間の距離によってファジィ分割を行うクラスタリング手法\cite{bezdek}を用いるものも多く提案されている．一方，伝統的な関数近似法としてのスプライン関数\cite{市田,桜井}や有限要素法\cite{菊地}では近似の精度を上げるために，非線形性の強い箇所のメッシュ分割が細分化される．本研究ではＩＤ３によって非線形性の強い部分をファジィ集合で細分化する方法を提案する．

本研究で取り上げるＢ−スプラインは局所台のある関数であり円形基底関数(Radial Basis Functions)と同様に受容野(Receptive Fields)のモデルや３層ニューラルネットのモデルとして，またファジィ推論ルールの集まりとして見なすことができる\cite{ichi2}．ＩＤ３にニューラルネットを融合したものには，決定木に線形しきい値ユニットを付加したUtgoff による Perceptron Tree \cite{utgoff}がある．これは決定木の葉節点(leaf node)が線形関数であるもので属性値に対してメンバシップ関数を定めるとファジィ制御などでよく知られている菅野ら\cite{sugeno}のファジィ推論の数式モデルとなる．また多層型ニューラルネットで決定木を表現したものに Sethi による Entropy Nets \cite{sethi}がある．これは決定木を表現している点でニューラルネットによりファジィ推論を表現したファジィニューラルネット\cite{furuhashi}と密接な関係がある．

ＩＤ３による決定木生成にファジィ集合を応用した研究は既にあるが，多次元写像の近似法としてその変数選択と構造決定にＩＤ３を用いるものは見あたらない．本研究では内部節点のない２次のＢ−スプラインを用いた特殊なスプライン関数であるBernstein多項式による関数近似法を取り上げる．多次元写像をBernstein多項式で近似しようとしたときに，各入力変数について２次以下の項からなる $n$ 変数多項式の項数は $3^n$ 個あり， $n$ が大きい場合はその中から組み合わせ最適化問題として選択するのは計算量が膨大となる．Bernstein多項式のような特殊な場合でなくても多変数スプラインの変数選択と節点の決定は非常に困難な問題である．ファジィ概念を導入して，スプライン関数の節点の決定に於けるヒューマンインタフェースを改善したシステムも提案されているが，一変数の場合に限られる\cite{yoshi}．

QuinianとRivest\cite{quin2}はシンボリックなデータを対象にしたＩＤ３で，情報量規準としてよく知られたＭＤＬＰ(Mininum Description Length Principle)を決定木生成の停止規則に用いている．木の生成の停止規則は未知パラメータ数の決定規則でもある．ニューラルネットのパラメータ数の決定にＭＤＬＰやＡＩＣ\cite{akaike,坂元}を用いた報告\cite{kurita}も知られているが，ローカルミニマムや結合重みの非一意性に起因する問題点も指摘されている\cite{wada,hagiwara}．

本論文では，提案した関数近似法としてのファジィＩＤ３が少ない計算コストでの有効なパラメータ選択法であることを，研削加工の実験データを用いて示し，多層型ニューラルネットワーク\cite{rumelhart}との比較や木生成の停止規則としてのＡＩＣの妥当性も検

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%  Section 2
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ＩＤ３}

決定木は獲得した知識がわかりやすく，ルールの抽出が容易であることから有効な知識表現の一つとされている．しかし，どの属性でデータ（オブジェクト，学習サンプル）を分類するかにより得られる決定木が異なるため，属性の選択基準が問題となる．
ＩＤ３\cite{quinlan} は決定木生成のための簡単かつ強力な手法であり，属性の選択基準として以下の情報量を採用している．

ＩＤ３では，あるノード $b$ において，データは相対頻度（確率） $p_k^b$ に従ってクラス $C_k$ に属すると考える．
\begin{eqnarray}
  p_k^b = \frac{|D_k^b|}
                {|D^b|}
\end{eqnarray}
ノード $b$ において，上記の確率より求められるエントロピー
\begin{eqnarray}
  H = - \sum_{k=1}^K p_k^b \log p_k^b
  \label{entropy}
\end{eqnarray}
をそのノードにおける情報量とする．ただし，$D^b$ はノード $b$ におけるデータ集合，$D_k^b$ は $D^b$ の中でクラス $C_k$ に属するデータ集合，$K$ はクラス数である．また，$|D|$ は集合 $D$ の濃度を表す．

今，決定木のノード$b$で，ある属性$A$が選択された場合に，その子ノード $b_v$ $(v=1,2,\cdots, V)$ における式（\ref{entropy}）の情報量を $H_v$ とする．ノード $b$ から子ノード $b_v$ に到達可能なデータの相対頻度 $q^{bv}$ を
\begin{eqnarray}　　　
  q^{bv} =  \frac{|D^{bv}|}
                 {|D^b|}
\end{eqnarray}
とする．ただし，$D^{bv}$ は子ノード $b^v$ に到達可能なデータ集合である．属性 $A$ を用いて分類した後の期待情報量 $E$ は
\begin{eqnarray}
  E = - \sum_{v=1}^V q^{bv} H_v
\end{eqnarray}
となる．ただし，$H_v$ は子ノード $b_v$ における情報量である．属性 $A$ を用いて分類することによって得られる獲得情報量 $G$ は
\begin{eqnarray}
  G=H-E
\end{eqnarray}
である．ＩＤ３ではこの獲得情報量が最大となる属性を用いてデータを分割する．

ＩＤ３で数値属性を取り扱うことができるファジィＩＤ３が提案されている\cite{馬野}． 本研究ではファジィ集合の直積をメンバシップ関数の積で定義し，
\begin{eqnarray}
  |D^b| = \sum_{\mbox{\boldmath $x$} \in D} 
                ( \prod_{(i,j) \in Q} \mu_{ij}(x_i) ) \\
  |D_k^b| = \sum_{\mbox{\boldmath $x$} \in D_k} 
                (  \prod_{(i,j) \in Q} \mu_{ij}(x_i) )
\end{eqnarray}
とする．ただし，$\mu_{ij}$ は第 $i$ 数値属性の第 $j$ メンバシップ関数，$(i,j)$ は属性 $i$ とその属性値 $x_i$ のファジィ集合の添字 $j$ の対を表しており，$Q$ はノード $b$ に到るまでに選択された $(i,j)$ の集合である．情報・決定理論\cite{miyazawa}やファジィ意思決定\cite{okuda}の分野では式(5)の獲得情報量は相互情報量とも呼ばれ非負の値をとる．式(5)の$G$が非負であるためには式(8)のファジィ情報源における事象系のファジィ直和条件が必要である\cite{okuda}．第 $i$ 属性の属性値のファジィ集合の数をJとして，
\begin{eqnarray}
   \sum_{j=1}^J \mu_{ij}(x_i)=1
\end{eqnarray}
であれば，属性値のファジィ集合の直積がメンバシップ関数の積（式(5),(6)）で定義されているので，各々のデータ（サンプル）についての多次元メンバシップ関数の値の総和は１となる．式(3)はファジィ集合 $b_v$ をファジィ事象とする確率（相対頻度）であるので，ファジィ直和条件から式(3)の $q^{bv}$ の $v$ についての和は1となり，式(1)の$p_k^b$（子ノード $b_v$ においては $p_k^{bv}$ ）の $k$ についての和も1であるので，これらを一種の確率として用いる．したがって，式(5)の非負性は次のように示される．
\begin{eqnarray}
  \sum_{v=1}^V p_k^{bv}q^{bv} = \sum_{v=1}^V \frac{|D_k^{bv}| \cdot |D^{bv}|}
                            {|D^{bv}| \cdot |D^{b}|} = p_k^b  
\end{eqnarray}
であるので，
\begin{eqnarray}
  G^b &=& -\sum_{k=1}^K p_k^b\log p_k^b -\sum_{v=1}^V q^{bv}(-\sum_{k=1}^K p_k^{bv}\log p_k^{bv}) \\
   &=& -\sum_{k=1}^K \sum_{v=1}^V p_k^{bv}q^{bv}\log p_k^b +\sum_{v=1}^V \sum_{k=1}^K p_k^{bv}q^{bv} \log p_k^{bv} \\
   &=& -\sum_{k=1}^K \sum_{v=1}^V p_k^{bv}q^{bv}\log \frac{p_k^b}
                                                           {p_k^{bv}} \\
& & \ge  -\sum_{k=1}^K \sum_{v=1}^V p_k^{bv}q^{bv} (\frac{p_k^b}
                                                         {p_k^{bv}} -1) \\
     &=&  0
\end{eqnarray}

クラスは簡単のためにクリスプ集合とする．ファジィＩＤ３ではクラスもファジィ集合として定められるのが一般的であるが，本研究では関数近似の問題を扱っているので，クラスは木の生成，すなわち，入力変数と多項式の項の選択に用いられているだけで，最終的なBernstein多項式には必要でない．クラスをファジィ集合としても，クリスプ集合としても木の生成には本質的な相違が無いと考えられるので，簡単化のためにクラスはクリスプ集合としている．
ファジィＩＤ３では決定木が複雑になることを防ぐために，木生成における二つの停止規則が設定されている\cite{馬野}． 一つは，あるノードにおいて相対頻度が最小占有率αを越えるクラスがある場合，もう一つは，あるノードにおけるデータ数である $|D^b|$ が最小データ数βよりも小さい場合である．

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section 3
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ｂ−スプライン・メンバシップ関数と勾配法による学習}


式(8)を満たすメンバシップ関数として本研究ではＢ−スプラインを用いる．三角型（屋根型）メンバシップ関数を用いる簡略化したファジィ推論モデル\cite{市橋,渡辺}は $C^0$ 級の区分的な多重線形関数であり，１次のＢ−スプラインは三角型メンバシップ関数と一致している．スプライン関数は区分的多項式による非線形補間の伝統的な手法である．図 \ref{spline} に示す内部節点を持たない２次のＢ−スプラインを用いると，以下の式（\ref{bernstein}）は Bernstein多項式となる．図 \ref{spline} で $\times$ 印は多重節点を示している．

２章での決定木は，ファジィ集合を用いるので，次のファジィ推論ルールに対応付けることができる．

 if  $x_i$ is $\mu_{11}$  and $x_2$ is $\mu_{21}$  $\cdots$  then $C_k$.
 
\hspace{10mm}     $\vdots$  \hspace{10mm}  $\vdots$ \hspace{10mm} $\vdots$ \hspace{10mm} $\vdots$ 

クラスはルールの結論部に相当するもので，クラス情報に代わって実数値のデータが与えられている場合は，ファジィルールの結論部を実数値としてニューラルネットの学習則（Least Mean Square, ＬＭＳ）によってその値を決定することもできる\cite{市橋,渡辺}．しかし，通常のＬＭＳ学習アルゴリズムでは学習係数は定数である．本研究では以下の最適化問題から得られる代数方程式の解を学習係数として用いる．まず，ファジィ推論モデルを以下のように表す．
\begin{eqnarray}
    y = \sum_{r=1}^R \mu_r w_r
    \label{bernstein}
\end{eqnarray}
\begin{eqnarray}
    \mu_r = \prod_{(i,j) \in Q_r} \mu_{ij}
\end{eqnarray}
ここで，$\mu_r$ は第 $r$ ルールの条件部に対応し，決定木ではルートから第 $r$ リーフにいたるまでの一次元のメンバシップ関数 $\mu_{ij}$ のすべての $(i,j)\in Q_r$ についての積で定義されている．したがって，直和条件から $\mu_r$ の $r$ についての総和は１である．また，$w_r$ は第 $r$ ルールにおける結論部実数値であり，$\mu_{ij}$ は内部節点のない２次のＢ−スプラインであるので，式（\ref{bernstein}）はBernstein多項式である．学習則を導出するために次の最小化問題を考える．
\begin{eqnarray}
  \min \ \     h(\Delta w) = \sum_{r=1}^R ( \Delta w_r)^{2s}  \\
  \hspace{10mm} s.t. \       \delta = \sum_{r=1}^R \mu_r \Delta w_r
  \label{prob1}
\end{eqnarray}
ただし，$\delta = y^*-y$ で，$y^*$ は理想出力（トレーニングサンプルの出力値，教師信号）である．$s$ は1, 2, 3,...の自然数から選ぶ．この問題は次のラグランジェ関数の最小化問題となり，
\begin{eqnarray}
  L(\Delta w) = \sum_{r=1}^R (\Delta w_r)^{2s} 
                + \lambda (\delta - \sum_{r=1}^R \mu_r \Delta w_r)
  \label{prob2}
\end{eqnarray}
\begin{eqnarray}
  \frac{\partial L}
       {\partial \Delta w_r} = 2s (\Delta w_r)^{2s-1} - \lambda \mu_r = 0 
       \ \        (r=1,2, \cdots, R) \\
  \frac{\partial L}
       {\partial \lambda} = \delta - \sum_{r=1}^R \mu_r \Delta w_r =0 
       \ \        (r=1,2, \cdots, R) 
\end{eqnarray}
となる．したがって，
\begin{eqnarray}
  \lambda = 2s
            \left(
               \frac{ \delta}
                   {\displaystyle \sum_{r=1}^{R} (\mu_r)^{\frac{2s}{2s-1}}} 
            \right)^{2s-1}
\end{eqnarray}
\begin{eqnarray}
  2s (\Delta w_r)^{2s-1} -
     2s \left(
         \frac{ \delta}
             {\displaystyle{\sum_{i=1}^R} (\mu_i)^{\frac{2s}{2s-1}}}
     \right)^{2s-1} \mu_r = 0
\end{eqnarray}
\begin{eqnarray}
  \Delta w_r = \frac{\delta (\mu_r)^{\frac{1}{2s-1}}}
                    {\displaystyle{\sum_{i=1}^R} (\mu_i)^{\frac{2s}{2s-1}}}
  \label{lms}
\end{eqnarray}
となる．通常のＬＭＳでは学習係数は一定であるが，この方法では学習係数はファジィルールの適合度合に依存する可変学習係数をもつことになる．学習則は
\begin{eqnarray}
 w_r^{NEW} = w_r^{OLD} + \tau \Delta w_r  \hspace{10mm} (r=1,2,\cdots, R)
\end{eqnarray}
となる．$s=1$ のとき式（\ref{lms}）の分母は $\|\nabla y \|^2$ であるので，逐次射影法\cite{fuku}を関数近似に用いる場合の学習則となっている．式(24)は地盤の断層撮影法\cite{geotomo}などに用いられている代数的再構成法(algebraic reconstruction technique)から式（\ref{prob2}）を定式化し学習則を求めたもので，パラメータに関して非線形な最小２乗法の場合にも非線形関数を逐次線形近似して用いることができる．この学習法は４章の数値例に示すように，パラメータ数に対して比較的データ数が少なく，トレーニングデータに対する誤差が小さくなる場合に学習の収束が早く有効である．４章の数値例では$\tau = 1$, $s = 1$ としている．

\begin{figure}[bt]
  \vspace*{80mm}
  \caption[内部節点のない２次のＢ−スプライン]{内部節点のない２次のＢ−スプライン}
  \label{spline}
\end{figure}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section 4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{実験データを用いた数値計算例}

研削加工における砥石摩耗は研削比Ｇ（＝被削材の除去体積／砥石の摩耗体積）
によって表される．
本研究ではこの研削比の値から決定木におけるクラス（上，中，下）を各クラスのデータ数が等しくなるように定めた．決定木を作成するにあたり以下の14個の属性（入力変数）を用いた．\\
・研削条件：（1）砥石周速，（2）送り\\
・砥石の仕様：（3）砥石の種類，（4）粒度，（5）結合度\\
・研削油剤の物理的性質と化学成分：（6）粘度，（7）比熱，（8）摩擦係数，（9）硫黄分\\
・被削材の化学成分と機械的性質：（10）炭素，（11）硫黄分，（12）クロム，（13）伸び，（14）ブリネル硬度 \\


%%%%%%%%%%%%%%%%%%表１
\begin{table}[t]
   \caption[研削データによる誤差の比較]{研削データによる誤差の比較}
   
   \vspace{3mm}
    
    (t):トレーニングデータ26件分の2乗誤差の和 \hfill \\
    (c):チェッキングデータ12件分の2乗誤差の和 \hfill \\

 \begin{center}
   \vspace{3mm}
   提案法 \\
   
   \begin{tabular}{cccc}  \hline 
     α  &    &   SSE  &  パラメータ数 \\   \hline 
     0.5 & (t)&  1.72  &   3  \\ 
         & (c)&  0.19  &      \\ 
     0.6 & (t)&  0.14  &   17 \\ 
         & (c)&  0.73  &      \\ 
     0.7 & (t)&  0.08  &   27 \\ 
         & (c)&  0.69  &      \\ 
     0.8 & (t)&  0.01  &   33 \\ 
         & (c)&  0.11  &      \\ 
     \underline{0.9} & \underline{(t)}&  \underline{0.00}  &   \underline{45} \\ 
         & \underline{(c)}&  \underline{0.12}  &    \\  \hline 
         &    & 学習回数 & 200    
    \end{tabular}

    \vspace{5mm}
    ニューラル・ネットワーク

    \begin{tabular}{cccc}   \hline  
      ユニット数  &    &   SSE  &  パラメータ数  \\ \hline 
      2  & (t) & 0.01  &    33  \\ 
         & (c) & 1.38  &        \\ 
      4  & (t) & 0.01  &    65  \\ 
         & (c) & 0.87  &        \\ 
      \underline{8}  & \underline{(t)} & \underline{0.00}  &   \underline{129}  \\ 
         & \underline{(c)} & \underline{0.48}  &        \\ 
      16 & (t) & 0.00  &   257  \\ 
         & (c) & 0.58  &        \\  \hline 
         &     & 学習回数 &  1000
    \end{tabular}
    \label{compare}
  \end{center}
\end{table}

%%%%%%%%%%%%%%%%%


38 件の実験データの約 1/3，すなわち 12 件をチェッキングデータとして選んだ．クラスはデータ件数がほぼ3等分されるように0.2と0.8をしきい値として３つ設定した．
最小データ数βを 1.0 とし，最小占有率αを 0.5 から 0.9 まで 0.1 ずつ増加させた．学習後の結果と中間層が一つの多層型ニューラル・ネットワークでの結果とを表 \ref{compare} に示す．ユニット数は中間層のユニット数である．表 \ref{compare} は誤差に関する比較で図 \ref{kensaku} は最小占有率αを 0.9 にしたときの出力$y$の比較を示している．最小占有率αを上げていくとトレーニングデータの誤差が小さくなり，最小占有率 0.9 では，20 回の学習
$(\tau=1, s=1)$ で 0.00 になった．チェッキングデータに対する誤差に関しても，ニューラル・ネットワークと同等な結果が得られた．十分な同定精度が得られるまで決定木の枝を増やしても，予測精度がそれほど悪くならず，汎化能力のあるルールの抽出（知識獲得）ができた．最大 $3^{14}$ 個の中から 45 個のファジィルール（パラメータ数に等しい）が選ばれている．


\begin{figure}[bt]
  \vspace*{95mm}
  \caption[研削比の推定値の比較]{研削比の推定値の比較}
  \label{kensaku}
\end{figure}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section 5
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{ＡＩＣによるスプライン係数の同定}

ファジィＩＤ３では決定木生成の停止規則に最小占有率αや最小データ数βが用いられるが，αやβの値を決めるための合理的な規準は存在しない．しかし，データの属性値やクラスが実数値として与えられていて，決定木の未知パラメータを線形の最小２乗法により推定する問題では，統計的なモデル選択の規準であるＡＩＣ\cite{akaike,坂元}の適用が可能である．ＡＩＣとはいくつかのモデルの候補が存在する場合に，モデルの最大対数尤度とパラメータ数から最良のモデルを決定する規準である．すなわち，

   ＡＩＣ＝−２×最大対数尤度＋２×パラメータ数

\noindent が最小となるモデルを選択する．ここでのパラメータ数にデータ数の対数を掛けたものを用いるＭＤＬＰもよく知られている．ニューラルネットの中間層におけるユニット数の決定にこれらの情報量規準が利用されているが\cite{kurita}，ローカルミニマムや結合重みの非一意性の理由からＡＩＣの有効性に否定的な意見もある\cite{wada,hagiwara}．３章の問題はs=1の場合，線形の最小２乗法の問題であるのでそのような問題点はないと考えられる．次の数値例では，スプライン関数のパラメータ（スプライン係数）同定に於けるＬＭＳ学習でのＡＩＣの妥当性を確認するために，モデルの汎化能力の客観的指標であると考えられる期待平均対数尤度を求めＡＩＣが同じパラメータ数を最適とするかを数値実験により検討する．

いま，入力ベクトル $x^i$ に対する出力 $z$ の条件付確率分布を $f(z|x^i,\theta)$，真の確率分布を $g(z|x^i)$ とする．それぞれの確率分布を以下のように正規分布で定義する．
\begin{eqnarray}
  f(z|x^i, \theta) \sim  N(y(x^i),\sigma^2) \\
  g(z|x^i, \theta) \sim  N(s(x^i),\eta^2) 
\end{eqnarray}
トレーニングデータ $x^i$ に対する条件付平均対数尤度は
\begin{eqnarray}
  l^*(\widehat{\theta}) &=& \int_{-\infty}^{\infty} 
                            g(z|x^i) \log f(z|x^i, \widehat{\theta}) dz 
                                                                \nonumber \\
                        &=& \frac{1}{2} \log 2 \pi \sigma^2 
                          - \frac{\eta^2 + (s(x^i) - y(x^i))^2}
                                 {2 \sigma^2}
\end{eqnarray}
である．ただし，$s(x^i)$ は真の分布の平均，$y(x^i)$ はモデルの分布の平均，$\eta^2$ は真の分布の分散，$\sigma^2$ はモデルの分布の分散である．すべてのトレーニングデータに対するこの値の平均値をとって条件付平均対数尤度とする．また，平均対数尤度の期待値である期待平均対数尤度
\begin{eqnarray}
  E_z[l^*(\widehat{\theta}) ] = \int_{-\infty}^{\infty} l^*(\widehat{\theta})
                                 \prod_{i=1}^n g(z| x^i) dz
\end{eqnarray}%% 95.06.11 修正　I => l  %%%%%%%%%%%%%%%%%%%%%%%%%%%
は，解析的に求めるのは困難なので，シミュレーションを複数回行い，各回について平均対数尤度を計算し，その平均とする．ＡＩＣの $-(1/2)$ 倍は $E_z$ のデータ件数倍の推定値である．


スプライン係数の同定にＬＭＳ学習を用いる場合に，ＡＩＣが利用可能であるかを，数値例を用いて検討した．ここでは，真のモデルを文献\cite{坂元}の例題より式（\ref{furiar}）のフーリエ級数に $N(0,1)$ に従う正規乱数をノイズとして加えたものとし，データ件数を変えて計算を行った．
\begin{eqnarray}
  S(x, \mbox{\boldmath $a$}) = a_0 +\sum_{m=1}^{10} ( a_{2m-1} \sin 2 m\pi x 
                                  + a_{2m} \cos 2m\pi x)
  \label{furiar}
\end{eqnarray}

\begin{tabular}{rrrrrrrr}
　$a_0 =$ & $8.000$    & $a_1 =$ &$ 2.415$   & $a_2 =$ &$-3.806$  & $a_3 =$&$ 2.119$ \\
\\
  $a_4 =$ & $-0.997$   & $a_5 =$ & $0.545$   & $a_6 =$ &$0.069$   & $a_7 =$&$-0.094$ \\
\\
  $a_8 =$ & $-0.078$   & $a_9 =$ & $-0.021$   & $a_{10}=$&$-0.065$ & $a_{11}=$&$-0.011$ \\
\\
  $a_{12}=$ & $0.042$  & $a_{13}=$ & $-0.011$ & $a_{14}=$ &$0.010$ & $a_{15}=$&$ 0.020$ \\
\\
  $a_{16}=$ & $-0.004$ & $a_{17}=$ & $0.002$ & $a_{18}=$ &$0.001$ & $a_{19}=$&$-0.006$ \\
\\
  $a_{20}=$ & $-0.009$ &                 &                 & & & &
\\
  & & & & & & & \\
\end{tabular}

%%%%%%%%%  表２のソースは文末　　table
%%%%%%%%%%%%   表２      %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[t]
 \begin{center}
    \caption[既知のモデル式を用いた比較]
            {既知のモデル式（\ref{furiar}）を用いた比較 \\}
    
    \begin{tabular}{ccccc|ccccc} \hline 
                   & \multicolumn{2}{c}{学習係数： 0.01}  
                   & \multicolumn{2}{c}{データ件数：500} &
                   & \multicolumn{2}{c}{学習係数： 0.03}  
                   & \multicolumn{2}{c}{データ件数：100} \\  \hline 
      パラメータ数 & $     7  $ &  $   12  $ & $  17   $ &  $  22   $ &
      パラメータ数 &  $    7  $ &  $   12  $ & $  17   $ &  $  22   $ \\
      最大対数尤度 & $ -762.07$ &  $-712.37$ & $-710.07$ &  $-708.88$ &
      最大対数尤度 &  $-151.35$ &  $-138.52$ & $-136.40$ &  $-135.91$ \\
      ＡＭＬ       & $ -764.08$ &  $\underline{-719.64}$ & $-720.17$ &  $-722.36$ &
      ＡＭＬ       &  $-154.34$ &  $\underline{-148.98}$ & $-151.99$ &  $-155.34$ \\ 
      ＡＩＣ       & $ 1538.15$ &  $\underline{1448.74}$ & $1454.14$ &  $1461.75$ &
      ＡＩＣ       &  $ 316.70$ &  $\underline{ 301.05}$ & $ 306.81$ &  $ 315.82$ \\ 
    \end{tabular}
                                                   
                                                   
    \begin{tabular}{ccccc|ccccc} \hline 
                   & \multicolumn{2}{c}{学習係数： 0.05}  
                   & \multicolumn{2}{c}{データ件数：50} &
                   & \multicolumn{2}{c}{学習係数： 0.07}  
                   & \multicolumn{2}{c}{データ件数：30} \\  \hline
      パラメータ数 &  $   7  $  &   $  12  $ & $ 17   $  &   $ 22   $ &
      パラメータ数 &  $   7  $  &   $  12  $ &  $ 17   $ &   $ 22   $  \\
      最大対数尤度 &  $-73.50$  &   $-67.99$ & $-65.64$  &   $-64.64$ &
      最大対数尤度 &  $-42.97$  &   $-37.89$ &  $-36.93$ &   $-35.05$  \\
      ＡＭＬ       &  $-78.25$  &   $\underline{-77.91}$ & $-81.62$  &   $-85.03$ &
      ＡＭＬ       &  $-51.58$  &   $\underline{-51.38}$ &  $-55.14$ &   $-58.20$ \\ 
      ＡＩＣ       &  $161.01$  &   $\underline{159.99}$ & $165.27$  &   $173.28$ &
      ＡＩＣ       &  $ 99.95$  &   $\underline{ 99.79}$ &  $107.87$ &   $114.10$ \\ 
    \end{tabular}
                                                     
                                                     
    \begin{tabular}{ccccc|ccccc} \hline
                   & \multicolumn{2}{c}{学習係数： 0.07}  
                   & \multicolumn{2}{c}{データ件数：20} &
                   & \multicolumn{2}{c}{学習係数： 0.07}  
                   & \multicolumn{2}{c}{データ件数：10} \\  \hline
      パラメータ数 &  $   3  $  &   $   7  $ &  $ 12   $ &   $ 17   $ &
      パラメータ数 &  $   3  $  &   $   7  $ &  $ 12   $ &   $ 17   $  \\
      最大対数尤度 &  $-54.20$  &   $-28.57$ &  $-26.07$ &   $-25.73$ &
      最大対数尤度 &  $-26.08$  &   $-18.02$ &  $-16.88$ &   $-15.06$  \\
      ＡＭＬ       &  $-56.81$  &   $\underline{-33.72}$ &  $-37.55$ &   $-40.06$ &
      ＡＭＬ       &  $-27.14$  &   $\underline{-19.26}$ &  $-19.74$ &   $-19.91$ \\ 
      ＡＩＣ       &  $122.80$  &   $\underline{ 71.13}$ &  $ 76.14$ &   $ 85.46$ & 
      ＡＩＣ       &  $ 64.68$  &   $\underline{ 50.05}$ &  $ 57.76$ &   $ 64.14$ \\ 
    \end{tabular}
                                                     
    \label{compare2}
 \end{center}
\end{table*}


未知パラメータ数は，二次のＢ−スプラインの数に等しい．$x$ の定義域の両端では３重節点とし，他の節点は等間隔に並べた．期待平均対数尤度は，同じ学習のシミュレーションを 100 回行い各回について平均対数尤度を計算し，その平均とした．１回のシミュレーションにつき，学習回数は 50 回とし，学習係数は 50 回以内にほぼ収束する値を選んだ．表 \ref{compare2} に計算結果を示す．ＡＩＣの $(-1/2)$ 倍が期待平均対数尤度のデータ件数倍（表 \ref{compare2} のＡＭＬ）の推定値であるが，データ数が 20 件以下の場合は少し誤差が大きい．しかし，どのデータ件数の場合も，期待平均対数尤度が最大のパラメータ数のところで，ＡＩＣの値が最小になっていることから，比較的データ数が少ない場合にもＡＩＣが有効な規範であるといえる．

次にＡＩＣを決定木生成の停止規則に用いた数値例を示す．





研削加工における表面あらさのデータを用いた．入力変数（属性）は $x_1$：切込み量(mm)と $x_2$：送り速度(mm/min)で，出力はＲmax：表面あらさ(μm)である．クラスはＲmaxの値から，0.4 と 0.7 を閾値として三つに設定した．データ数は 14 件である．図 \ref{spline} の三つのＢ−スプラインをメンバシップ関数として用いた．種々のパラメータ数の木に対するＡＩＣの値を表 \ref{aic} に示す．ただし，ＡＩＣでのパラメータ数は表 \ref{aic} のパラメータ数に１を加えたものである．各パラメータ数に対し，それぞれ２，６，６，１通りの木のパターン（ファジィパーティション）があるが，表 \ref{aic} はそれぞれの最小値を示している．たとえばパラメータ数が５のとき，図 \ref{partition} に示すように６通りの木に対応する２次元平面上のファジィパーティションが考えられ，各々のＡＩＣの値を表 \ref{aic1} に示す．

ＡＩＣの値が最小となる a) の場合が最適であるが（パラメータ数が等しいので２乗誤差が最小となっている）；ＩＤ３により決定木を生成すると図 \ref{partition} の a) に対応する決定木が得られた．図 \ref{tree} にその決定木を示す．


%%%%%%%%%%%%%%%%%%%%%   表３ ４  %%%%%%%%%%%%%%%%%%%%%5
\begin{table}[t]
 \begin{center}
    \caption[各パラメータ数に対するＡＩＣの値]
            {各パラメータ数に対するＡＩＣの値}

    \begin{tabular}{cccccc} \hline 
      パラメータ数 & $ 3   $ &  $5   $ & $ 7   $ & $ 9   $ &  (25)  \\  \hline 
      最大対数尤度 & $-1.48$ &  $0.81$ & $ 0.82$ & $ 0.82$ & $2.43$ \\ \\
      ＡＩＣ       & $ \underline{8.96}$ &  $8.38$ & $12.35$ & $16.35$ & $ 45.14$ \\ \hline 
    \end{tabular}
 \label{aic}
 \end{center}
\end{table}


\begin{table}[t]
 \begin{center}
    \caption[パラメータ数が5の場合のＡＩＣ]
            {パラメータ数が5の場合のＡＩＣ}
    \begin{tabular}{ccccccc} \hline 
         &  a)  &   b)   &  c)   &  d)   &   e)   &   f)   \\ \hline  
     ＡＩＣ & \underline{8.38} &  12.70 &  9.45 &  8.39 &  13.76 &  14.19 \\ \hline  
    \end{tabular}
 \label{aic1}
 \end{center}
\end{table}

%%%%%%%%%%%%%%%%%%%%%





\begin{figure}[t]
  \vspace*{80mm}
  \caption[パラメータ数５のファジィパーティション]
  {パラメータ数５のファジィパーティション}
  \label{partition}
\end{figure}

\begin{figure}[t]
  \vspace*{70mm}
  \caption[得られた決定木]
  {得られた決定木}
  \label{tree}
\end{figure}

\begin{figure}[t]
  \vspace*{110mm}
  \caption[得られた近似関数]
  {得られた近似関数}
  \label{arasa}
\end{figure}



また，パラメータ数が３と７の場合に対してもＩＤ３によって得られる決定木はＡＩＣが最小となる場合と一致した．すなわち，パラメータ数が等しいときにＩＤ３によって得られるモデルとＡＩＣ規準によって選択されるモデル（この場合は２乗誤差による選択に等しい）とが一致した．表\ref{aic}よりＡＩＣ基準によって選択される木はパラメータ数５の場合であり，このようにしてＡＩＣを決定木生成の際の停止規則として用いることができる．図 \ref{arasa} は3000 回の学習$(\tau = 0.01, s = 1)$を行った後の同定されたモデル（Bernstein多項式）を示したもので，・印は実験データを示している．図 \ref{arasa} では，トレーニングデータのない領域でも極端な変化がなく，ある程度うまく外挿できている．


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Section ６
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{おわりに}

本研究では，ファジィＩＤ３により得られる決定木をファジィルールの集合と考え，学習係数を可変としたＬＭＳ学習を用いてファジィルールの結論部を決定する手順について述べた．またBernstein多項式として表されたファジィ推論モデルのパラメータ同定におけるＡＩＣの有効性を検討した．トレーニングデータ数が少ない場合には，ＡＩＣは期待平均対数尤度の推定値としては十分なものではないが，モデルの選択規範すなわち決定木生成の停止規則としては有効であることが確認できた．本提案法はBernstein多項式による簡便なモデル同定法であり，かつファジィルールとしてルールを抽出できる知識獲得手法である．本論文では，Bernstein多項式の場合について述べたが，提案法においてＢ−スプラインの数，すなわち節点数を増やせば一般的なスプライン関数となるので，多変数スプラインにも応用できる．しかし，その場合は節点の数だけでなく位置も適切に決定する必要がある．また，属性値が未知なデータやシンボリックなデータを含む場合への対応などが今後の課題として残されている．



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Reference  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%  参考文献
\begin{thebibliography}{99}

\bibitem{maeda}
  前田章：
  ``データベースからの知識発見技術''，
  システム／制御／情報，
  pp. 185-190,
  Vol. 39,
  No. 4, 
  (1995)

\bibitem{quinlan}
 Quinlan, J. R.:
 ``Induction of Decision Tree,'' 
 Machine Learning, 
 pp.81-106, 
 Vol. 1, 
 (1986)
 
\bibitem{forsyth}
  Forsyth, R. and Rada, R:
  Machine Learning: Applications in Expert Systems and Information Retrieval, 
  Ellis Horwood Limited,
  (1988)


\bibitem{馬野}
  Umano, M. {\it et al.}:
  ``Generation of Fuzzy Decision Trees by Fuzzy ID3 Algorithm and Its Application to Diagnosis by Gas in Oil,''
  Proc. 1994 Japan-U.S.A. Symposium on Flexible Automation, 
  pp.1445-1448,
  (1994)
  
\bibitem{sugeno}
  Sugeno, M. and Kang, G. T.:
  ``Structure Identification of Fuzzy Model,''
  Fuzzy Sets and Systems, 
  pp.15-33,
  Vol.28,
  (1988)
  
\bibitem{ivakhnenko}
  Ivakhnenko, A. G.:
  ``The Group Method of Data Handling, a Rival of the Method of Stochastic Approximation,''
  Soviet Automatic Control,
  pp.43-55,
  Vol. 13,
  (1968)
  
\bibitem{中森}
  中森義輝，領家美奈：
  ``ファジィモデリング再考''，
  日本ファジィ学会誌,
  pp. 453-464,
  Vol. 5,
  No. 3,
  (1993)

\bibitem{bezdek}
  Bezdek, J. C.:
  Pattern Recognition with Fuzzy  Objective Function Algorithms,
  Plenum Press,
  (1981)
  
\bibitem{市田}
  市田浩三，吉本富士市：
  スプライン関数とその応用,
  教育出版
  (1979)
  
\bibitem{桜井}
  桜井明(監修)：
  「Ｃによるスプライン関数」，
  東京電機大学出版局，
  (1993)
  
\bibitem{菊地}
  菊地文雄，岡部政之：
  「有限要素システム入門」，
  日科技連，
  (1986)

\bibitem{ichi2}
  市橋秀友，佐藤禎浩，三好哲也，長坂一徳：
  ``ニューロ・ファジィ有限要素法と誤差解析''，
  システム制御情報学会論文誌，
  pp. 122-129,
  Vol. 8,
  No. 3, 
  (1995)

\bibitem{utgoff}
  Utgoff, P. E.:
  ``Perceptron Trees: A Case Study in Hybrid Concept Representation,''
  Connection Science,
  pp.377-391,
  Vol.1,
  No.4,
  (1989)

\bibitem{sethi}
  Sethi, I. K.:
  ``Entropy Nets: From Dcision Trees to Neural Networks,''
  Proc. IEEE, 
  pp. 1605-1613,
  Vol.78,
  No.10,
  (1990)
  
\bibitem{furuhashi}
  Horikawa, S., et al.:
  ``On Fuzzy Modeling Using Fuzzy Neural Networks with the Back-Propagation Algorithm,''
  IEEE Trans. on Neural Networks, 
  pp. 801-806,
  Vol.3,
  No.5,
  (1992)

\bibitem{yoshi}
  吉本富士一：
  ``ファジィ概念を用いたスプライン関数の節点の決定''，
  情報処理学会論文誌，
  pp. 1682-1690,
  Vol. 35,
  No. 9, 
  (1994)

\bibitem{quin2}
  Quinlan, J. R. and Rivest, R. L.:
  ``Inferring Decision Trees Using the Minimum Description Length Principle,''
  Information and Control, 
  pp. 249-268,
  Vol.80,
  (1989)

\bibitem{akaike}
  Akaike, H.:
  ``A New Look at the Statistical Identification,''
  IEEE Trans. Autom. Contr.,
  AC-19,
  pp.716-723,
  (1974)
  
\bibitem{坂元}
  坂元慶行，石黒真木夫，北川源四郎：
  「情報量統計学」，
  共立出版,
  (1983)
  
\bibitem{kurita}
  栗田多喜夫：
  ``情報量基準による３層ニューラルネットワークの隠れ層のユニット数の決定法''，
  電子情報通信学会論文誌，
  pp. 1872-1878,
  Vol. J73-D-II,
  No. 11, 
  (1990)

\bibitem{wada}
  和田安弘，川人光男：
  ``新しい情報量基準とCross Validationによる汎化能力の推定''，
  電子情報通信学会論文誌，
  pp. 955-965,
  Vol. J74-D-II,
  No. 7, 
  (1991)

\bibitem{hagiwara}
  萩原克幸，戸田尚宏，臼井支朗：
  ``階層型ニューラルネットワークにおける結合重みの非一意性とAIC''，
  電子情報通信学会論文誌,
  pp. 2058-2065,
  Vol. J76-D-II,
  No. 9,
  (1993)

\bibitem{rumelhart}
  Rumelhart, D. E., McClelland, J. L. and the PDP Research Group:
  Parallel Distributed Processing,
  Cambridge,MA:MIT Press,
  (1987)
  
\bibitem{miyazawa}
  宮沢光一：
  情報・決定理論序説,
  岩波書店
  (1976)
  
\bibitem{okuda}
  奥田徹二：
  ファジィＯＲ（日本ファジィ学会編）第２章ファジィ意思決定,
  日刊工業新聞社
  (1993)
  
\bibitem{市橋}
  市橋秀友：
  ``ファジィ制御とモデリング''，
  システム／制御／情報，
  pp. 30-37,
  Vol. 37,
  No. 1, 
  (1993)
  
\bibitem{渡辺}
  市橋秀友：
  ``ニューロ的手法によるファジィルールの調整''，
  日本ファジィ学会誌,
  pp. 191-203,
  Vol. 5,
  No. 2,
  (1993)
  
\bibitem{fuku}
  巽啓司，福島雅夫：
  ``逐次射影法としての誤差逆伝搬法''，
  システム制御情報学会論文誌,
  pp. 204-211,
  Vol. 8,
  No. 5,
  (1995)
  
\bibitem{geotomo}
  Dines,K.A. and Lytle,R.J.:
  ``Computerized Geophysical Tomography'',
  Proc. of IEEE,
  pp.1065-1073
  Vol.67, 
  No.7, 
  (1979)
  

\end{thebibliography}




\end{document}


