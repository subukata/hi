\documentclass{article}
\usepackage{graphicx}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% New Command and Environment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% boldmath define
\newcommand{\bmath}[1]{\mbox{\boldmath $#1$}}
\newcommand{\vctr}[1]{\mbox{\boldmath $#1$}}




\newtheorem{theorem}{Theorem}
\setcounter{page}{1}

\begin{document}

\title{Nonlinear Discriminant Analysis of Categorical Data by Fuzzy $c$-Means Fixed Point Iteration
\thanks{Proc. of Joint 1st International Conference on Soft Computing and Intelligent Systems and 3rd International Symposium on Advanced Intelligent Systems, \#24B4-1 (2002)}
}

\author{Hidetomo Ichihashi, Ryuichi Niiyama, Katsuhiro Honda, Chi-Hyon Oh\\
        Graduate School of Engineering, Osaka Prefecture University\\
1-1 Gakuencho, Sakai, Osaka 599-8531 Japan\\
        ichi@ie.osakafu-u.ac.jp\\}

\date{}

\maketitle
\thispagestyle{plain}\pagestyle{plain}

\begin{abstract}
In this paper, we introduce memberships in fuzzy clusters to the discriminant analysis for categorical data, which is known as Hayashi's second quantification method. Cluster partition is unknown and is determined by the combined algorithm of the Hayashi's discriminant analysis and the Fuzzy $c$-Means(FCM) clustering. The multiplication of unknown memberships makes the underlying models nonlinear ones. Contrary to FCM, the clustering criterion is not the distance from cluster center but is the between group variation. The fuzzy clusters are implicitly used for classification of new data.
\end{abstract}

Keywords: 
Fuzzy clustering, categorical data, quantifying method, nonlinear discriminant analysis
\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% section I   Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Nominal measurement consists of assigning objects to categories. Variables measured on a nominal scale are often referred to as categorical or qualitative variables. The discriminant analysis for categorical data is known as Hayashi's second quantification method~\cite{Hayashi}. Individuals usually respond to a questionnaire by selecting category in each item of questions. Individuals are divided into some groups or classes by their respective external criteria or characters. The objective of the analysis is to predict a group to which a newly given individual belongs. Like other data analysis methods, many of the quantification methods for categorical data are developed on a linear model. In this paper we introduce memberships in fuzzy clusters to the discriminant analysis for categorical data. We use the term "cluster" separately from class or group. Cluster partition is unknown to all individuals and is determined by the combined algorithm of discriminant and cluster analysis. The multiplication of memberships (i.e., unknown parameters) makes the underlying models nonlinear ones. Fuzzy $c$-Means (FCM) clustering by Bezdek {\it et al}.~\cite{Bezdek} is the popular method that partitions a data set into fuzzy clusters. Though we adopt this approach for clustering, the clustering criterion is not the distance from cluster center but the between group variation. In our approach, the groups or classes are crisp but the clusters are fuzzy, and the fuzzy clusters are implicitly used for classification of new individuals. Numerical example shows improvement in classification performance.

%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simultaneous Application of Clustering and Quantification Method}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[t]
	\caption{Data format for discriminant of categorical responses} 
        \vspace{3mm}
	\label{tab:q_data_ext}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|} \hline
			\multicolumn{1}{|c|}{group }   & \multicolumn{1}{c|}{individual}     & \multicolumn{3}{c|}{item 1}                                                        & \multicolumn{3}{c|}{\ldots}                         & \multicolumn{3}{c|}{item $R$}                                                        \\\cline{3-11}
			\multicolumn{1}{|c|}{number}   & \multicolumn{1}{c|}{}      & \multicolumn{3}{c|}{\hspace{1mm}1 \hspace{3mm}\ldots \hspace{3mm}$x_1$}  & \multicolumn{3}{c|}{\ldots}                        & \multicolumn{3}{c|}{\hspace{1mm}1  \hspace{3mm}\ldots  \hspace{3mm}$x_R$}   \\\hline
			\multicolumn{1}{|c|}{}        & \multicolumn{1}{c|}{1}       & \multicolumn{3}{c|}{\hspace{-15mm}$\surd$}                                        & \multicolumn{3}{c|}{}                               & \multicolumn{3}{c|}{\hspace{-15mm}$\surd$}                                          \\
                                \multicolumn{1}{|c|}1   & \multicolumn{1}{c|}{\vdots} & \multicolumn{3}{c|}{\hspace{-1mm}$\surd$}                                         & \multicolumn{3}{c|}{\vdots}                              & \multicolumn{3}{c|}{\hspace{-15mm}$\surd$}                                          \\
                                \multicolumn{1}{|c|}{}        & \multicolumn{1}{c|}{$n_2$}  & \multicolumn{3}{c|}{\hspace{-15mm}}                                                 & \multicolumn{3}{c|}{\hspace{-15mm} }          & \multicolumn{3}{c|}{\hspace{-15mm}}                                                   \\\hline
			\multicolumn{1}{|c|}{\vdots} & \multicolumn{1}{c|}{\vdots} & \multicolumn{3}{c|}{\hspace{-15mm}}                                                 & \multicolumn{3}{c|}{\hspace{-15mm} }          & \multicolumn{3}{c|}{\hspace{-15mm}}                                                  \\\hline
                                \multicolumn{1}{|c|}{}        & \multicolumn{1}{c|}{$1$}     & \multicolumn{3}{c|}{\hspace{-15mm} $\surd$}                                       & \multicolumn{3}{c|}{ }                             & \multicolumn{3}{c|}{\hspace{15mm}$\surd$}                                            \\
			\multicolumn{1}{|c|}{$K$}    & \multicolumn{1}{c|}{\vdots} & \multicolumn{3}{c|}{\vdots}                                                             & \multicolumn{3}{c|}{\vdots}                       & \multicolumn{3}{c|}{\vdots}                                                               \\
			\multicolumn{1}{|c|}{}        & \multicolumn{1}{c|}{$n_k$}  & \multicolumn{3}{c|}{\hspace{15mm}$\surd$}                                          & \multicolumn{3}{c|}{ }                              & \multicolumn{3}{c|}{\hspace{15mm} $\surd$}                                          \\\hline
		\end{tabular}
	\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Table \ref{tab:q_data_ext} shows the example of questionnaire for discriminant analysis of categorical data. Responses of each individual are listed and individuals are divided into $K$ classes. To represent the responses we introduce following dummy variables.
%
\begin{equation}
	\delta_ip(jk)=
	\left\{
	\begin{array}{lc}
		1 \  \  \ {\rm individual} \ $p$ \ {\rm responded} \\
		\hspace{5mm}{\rm to \ category} \ $k$ \ {\rm of \ item \ }$j$ \\
		0 \  \  \ {\rm else}
	\end{array}
	\right.
	\label{eq:damy}
\end{equation}
%
Each individual is given a value $Y_{cip}$ by a weighted linear sum of the dummy variables as: 
\begin{eqnarray}
	Y_{cip} = \sum_{j=1}^{R} \sum_{k=1}^{x_{j}} a_{cjk} \delta_{ip}(jk)
\label{eq:predict}
\end{eqnarray}

In Eq.(\ref{eq:predict}), coefficients $a_{cjk}$ denote the weight for category $k$ of item $j$ in cluster $c$. It should be noted that the individuals are divided into $K$ groups or classes. The group of each individual is assumed to be known and its membership to the group is crisp. The individuals are also partitioned into $C$ clusters that are unknown and the memberships are fuzzy. The membership values are assigned by the fixed point iteration algorithm in FCM. To clarify the differences of meaning between the "class" and "cluster", we use the term "group" instead of "class". The objective function is formulated so as to discriminate all individuals into $K$ groups by the values of Eq.(\ref{eq:predict}). For this purpose Hayashi's second quantification method maximizes the between-group variation against the within-group variation and the weight coefficients $a_{cjk}$ are determined to attain this goal. We follow this approach and introduce membership $u_{cip}$ to change the linear model Eq.(\ref{eq:predict}) into nonlinear one. The subscripts $c,i$ and $p$ represent the cluster number, group number and individual number respectively.

Though we introduce the membership $u_{cip}$, the variation of $Y_{cip}$ can be decomposed as is known in the analysis of variance (ANOVA).
\begin{eqnarray}
\sum_{c=1}^C \sum_{i=1}^K \sum_{p=1}^{n_i} u_{cip} (Y_{cip}-\bar{Y}_c)^2 \nonumber \\
   = \sum_{c=1}^C \sum_{i=1}^K n_{ci} (\bar{Y}_{ci}-\bar{Y}_c)^2
 \nonumber
\end{eqnarray}
\vspace{-3mm}
\begin{eqnarray}
\hspace{9mm} + \sum_{c=1}^C \sum_{i=1}^K \sum_{p=1}^{n_i} u_{cip} (Y_{cip}-\bar{Y}_{ci})^2
	\label{eq:bunsanbunseki}
\end{eqnarray}
The first and the second terms of the right hand side represent the between-group variation $S_{B_c}$ and the within-group variation $S_{W_c}$.
$u_{cip}$ is constrained by
%
\begin{eqnarray}
	\sum_{c=1}^C u_{cip}= 1, \quad u_{cip} \in [ 0,1 ], \quad
	p=1, \cdots , n_i, i =1, \cdots , k
	\label{eq:membership} 
\end{eqnarray}
%
$C$ denotes the total number of clusters.
$n_{ci}$ is the sum of the memberships of individuals in group $i$.
\begin{eqnarray}
\hspace{-7mm} n_{ci} = \sum_{p=1}^{n_i} u_{cip}
\label{eq:n_ci}
\end{eqnarray}
%
$\bar{Y}_{c}$ is the grand mean of $Y_{cip}$ in cluster c.
%
\begin{eqnarray}
\bar{Y}_{c}=\frac{\displaystyle{\sum_{i=1}^K}\displaystyle{\sum_{p=1}^{n_i}}u_{cip}Y_{cip}}{\displaystyle{\sum_{i=1}^K}\displaystyle{\sum_{p=1}^{n_i}}u_{cip}}=\frac{\displaystyle{\sum_{j=1}^R}\displaystyle{\sum_{k=1}^{x_j}}n_{cjk}a_{cjk}}{n_{c}}
\label{eq:Y_c}
\end{eqnarray}
%
$n_{cjk}$ is the sum of the memberships of individuals who responded to the category  $k$ of the item $j$.
%
\begin{eqnarray}
\hspace{-7mm} n_{cjk} = \sum_{i=1}^{K}\sum_{p=1}^{n_i} u_{cip}\delta_{ip}(jk)
\label{eq:n_cjk}
\end{eqnarray}
%
$\bar{Y}_{ci}$ is the within group mean in each cluster. 
%
\begin{eqnarray}
	\bar{Y}_{ci}=\frac{\displaystyle{\sum_{p=1}^{n_i}}u_{cip}Y_{cip}}{\displaystyle{\sum_{p=1}^{n_i}}u_{cip}}=\frac{\displaystyle{\sum_{j=1}^R}\sum_{k=1}^{x_j}g_c^i(jk)a_{cjk}}{n_{ci}}
	\label{eq:Y_ci}
\end{eqnarray}
%
$g_c^i(jk)$ is the sum of the memberships in cluster $c$ of individuals in group $i$, that responded to category $k$ of item $j$.
\begin{eqnarray}
	g_c^i(jk)=\sum_{p=1}^{n_i}u_{cip}\delta_{ip}(jk)
	\label{eq:g}
\end{eqnarray}
By substituting Eqs.(\ref{eq:predict}), (\ref{eq:Y_c}) and (\ref{eq:Y_ci}) into each term of Eq.(\ref{eq:bunsanbunseki}) and rearranging, The variation $S_{T_c}$ in cluster $c$ and the variation between groups $S_{B_c}$ can be written respectively as:
\begin{eqnarray}
S_{T_c} = \sum_{c=1}^C\sum_{i=1}^K \sum_{p=1}^{n_i} u_{cip}(Y_{cip}-\bar{Y_c})^2 \nonumber 
\end{eqnarray}
\begin{eqnarray}
\hspace{7mm} = \sum_{c=1}^C \sum_{j=1}^R \sum_{k=1}^{x_j} \sum_{l=1}^R \sum_{m=1}^{x_l} t_c(jk,lm)a_{cjk}a_{clm} \nonumber
\end{eqnarray}
\begin{eqnarray}
\hspace{7mm} = \vctr{a_c}{^T}T_c \vctr{a_c}
\label{eq:S_t} 
\end{eqnarray}
\begin{eqnarray}
 t_c(jk,lm) = \{ f_c(jk,lm)- \frac{n_{cjk}n_{clm}}{n_c} \}  \label{eq:t_c} 
\end{eqnarray}
%
and
%
\begin{eqnarray}
S_{B_c} = \sum_{c=1}^C \sum_{i=1}^Kn_{ci}(\bar{Y}_{ci}-\bar{Y_c})^2 \nonumber \nonumber 
\end{eqnarray}
\begin{eqnarray}
\hspace{7mm} = \sum_{c=1}^C \sum_{j=1}^R \sum_{k=1}^{x_j} \sum_{l=1}^R \sum_{m=1}^{x_j} b_c(jk,lm) a_{cjk}a_{clm}\nonumber
\end{eqnarray}
\begin{eqnarray}
\hspace{7mm} = \vctr{a_c}{^T}B_c \vctr{a_c}
\label{eq:S_b} 
\end{eqnarray}
\begin{eqnarray}
 b_c(jk,lm) = \{ \sum_{i=1}^K \frac{g_c^i(jk)g_c^i(lm)}{n_{ci}}- \frac{n_{cjk}n_{clm}}{n_c} \}  \label{eq:b_c} 
\end{eqnarray}

$f_c(jk,lm)$ in Eq.(\ref{eq:t_c}) is the sum of the memberships of individuals who responded to both the item $j$, category $k$ and the item $l$, category $m$. 
\begin{eqnarray}
 f_c(jk,lm) = \sum_{i=1}^K \sum_{p=1}^{n_i} u_{cip}\delta_{ip}(lm)\delta_{ip}(jl)  \label{eq:f} 
\end{eqnarray}
Hence, the objective function for maximizing the variations between-groups and partitioning into clusters can be given as follows:
\begin{eqnarray}
	\L =&& \sum_{c=1}^C \sum_{i=1}^K \sum_{p=1}^{n_i} u_{cip} (\bar{Y_{ci}}-\bar{Y_c})^2 \nonumber \\
&&- \lambda_c( \sum_{c=1}^C \sum_{i=1}^K \sum_{p=1}^{n_i} u_{cip} (Y_{cip}-\bar{Y_c})^2-1) \nonumber \\
&&-\tau_0 \sum_{c=1}^C \sum_{i=1}^K \sum_{p=1}^{n_i} u_{cip} \log u_{cip} \nonumber \\
&&-\sum_{i=1}^{K} \sum_{p=1}^{n_i} \tau_{ip} (\sum_{c=1}^{C}u_{cip}-1) \nonumber \\
=&& \vctr{a_c}{^T}B_c \vctr{a_c} - \lambda_c(\vctr{a_c}{^T}T_c \vctr{a_c}-1) 
\nonumber \\
&&-\tau_0 \sum_{c=1}^C \sum_{i=1}^K \sum_{p=1}^{n_i}u_{cip} \log u_{cip} \nonumber \\
&&-\sum_{i=1}^{K} \sum_{p=1}^{n_i} \tau_{ip} (\sum_{c=1}^{C}u_{cip}-1) 
\label{eq:objective} 
\end{eqnarray}


The third term of Eq.(\ref{eq:objective}) is called entropy term, which serves as a regularizer~\cite{Miyamoto} in Fuzzy $c$-Means Clustering. The coefficient $\tau_0$ is for regulating the fuzziness. $\tau_{ip}$ is the Lagrangean multiplier.

From the optimality condition $\partial L/\partial \vctr{a_c} = 0$, we have 
\begin{eqnarray}
	(B_c-\lambda_{c}T_c)\vctr{a_c}=\vctr{0}
        \label{eq:eigen}
\end{eqnarray}
It is assumed that each individual responds to a single category within each item.
Thus, for all $j$, $i$ and $p$, following relation holds. 
\begin{equation}
\sum_{k=1}^{x_j}\delta_{ip}(jk)=1 \hspace{3mm} 
\end{equation}
So, as convention we set
\begin{equation}
a_{cj1} = 0,\hspace{6mm} c=1, \cdots, C,\hspace{3mm}j=1, \cdots, R
\end{equation}
Thus, by eliminating the rows and columns corresponding to the first category in each item from the matrices $B_c$ and $T_c$ in Eq. (\ref{eq:eigen}), we solve Eq. (\ref{eq:eigen}) for each cluster. The solution $a_{cjk}$ is the eigenvector corresponding to the largest eigenvalue $\lambda_c$ in Eq.(\ref{eq:eigen}).
%
\begin{equation}
\lambda_c = \frac{\vctr{a}_c^{T}B_c\vctr{a}_c}{\vctr{a}_c^{T}T_c\vctr{a}_c}
\end{equation}
In the first step, $a_{cjk}$ is normalized such that
\begin{equation}
 || \vctr{a}_c || = 1
\label{eq:1}
\end{equation}
and then taking the volume of each cluster into account 
\begin{equation}
 \acute{a}_{cjk} = \sqrt[]{n_c}a_{cjk}
\end{equation}
%
\begin{equation}
a_{cjk}^{*}=\acute{a}_{cjk} - \overline{Item}_{cj}, \nonumber 
\end{equation}
\begin{equation}
\overline{Item}_{cj} = \frac{1}{n_c}\sum_{k=1}^{x_j}n_{cjk}\acute{a}_{cjk}
\label{eq:standardize_a}
\end{equation}
$\overline{Item}_{cj}$ is the mean value of $\acute{a}_{cjk}$ within item $j$ in cluster $c$.
Thus, we have
\begin{equation}
\bar{Y}_c = \frac{1}{n_c}\sum_{j=1}^{R}\sum_{k=1}^{x_j}n_{cjk}(a_{cjk}^{*} + \overline{Item}_{cj})
\end{equation}
\begin{equation}
\bar{Y}_{ci} = \frac{1}{n_{ci}}\sum_{j=1}^{R}\sum_{k=1}^{x_j}g_c^i(jk)(a_{cjk}^{*} + \overline{Item}_{cj})
\end{equation}
\begin{equation}
Y_{cip} = \sum_{j=1}^{R}\sum_{k=1}^{x_j}(a_{cjk}+\overline{Item}_{cj})\delta_{ip}(jk) \label{eq:ex_quan}
\end{equation}
From ${\partial L}/{\partial u_{cip}} = 0$, ${\partial L}/{\partial \tau_{ip}} = 0$
\begin{eqnarray}
	u_{cip} = \frac{\exp (A_{cip})}{\displaystyle{\sum_{c^{'}=1}^C} \exp( A_{c^{'}ip})}
        \label{eq:u}
\end{eqnarray}
where
{\small
\begin{eqnarray}
A_{c^{'}ip} = \frac{1}{\tau_0}(\bar{Y}_{c^{'}i} - \bar{Y}_{c^{'}})^2 - \frac{\lambda_{c^{'}}}{\tau_0} (Y_{c^{'}ip} - \bar{Y}_{c^{'}})^2 \label{eq:ei}
\end{eqnarray}
}

The solution to the problem, which maximize the objective function, is obtaind through the fixed point iteration like in the conventional Fuzzy $c$-Means clustering.
Following is the algorithm.
\begin{description}
\item[(Step 1)] Set $\tau_0$, $C$ and small positive number $\varepsilon$. Choose initial values of the membership $u_{cip}$ randomly from unit interval [0, 1].
\item[(Step 2)] Solve eigenvalue problem (\ref{eq:eigen}) for each cluster and obtain $a_{cjk}$.
\item[(Step 3)] Normalize $a_{cjk}$ by Eqs.(\ref{eq:1}) to (\ref{eq:standardize_a}).
\item[(Step 4)] Update membership $u_{cip}$ by Eq.(\ref{eq:u}).
\item[(Step 5)] If the following termination condition is satisfied,
\begin{equation}
	\mathop{\mbox{max}}_{c,i,p} \{ | u_{cip}^{NEW} - u_{cip}^{OLD} |\} < \varepsilon
\end{equation}
then stop, otherwise go to $(Step \ 2)$.
\end{description}

From the obtained $a_{cjk}^{*}$, each group mean is given as:
\begin{eqnarray}
b_{ci} = \frac{1}{n_{ci}}\sum_{p=1}^{n_i}Y_{cip}, \nonumber
\end{eqnarray}
\vspace{-3mm}
\begin{eqnarray}
  \hspace{5mm}       c=1, \cdots, C, i=1 , \cdots, K
        \label{eq:b}
\end{eqnarray}
And, $b_{ci}$ is also normalized as is done for $a_{cjk}$.
\begin{equation}
b_{ci}^{*} = b_{ci} - \overline{exY}_{c}, \nonumber 
\end{equation}
\begin{equation}
\overline{exY}_{c} = \frac{1}{n_c}\sum_{i=1}^{K}\sum_{p=1}^{n_i}u_{cip}Y_{cip}
\label{eq:standardize_b}
\end{equation}
$\overline{exY}_{c}$ is the grand mean in each cluster.
It is also necessary to normalize $Y_{cip}$ such that
\begin{equation}
Y_{cip}^{*} = Y_{cip} - \overline{exY}_{c}
\label{eq:standardize_Y}
\end{equation}
For the discriminant or classification of new individuals, the mean value $b_{ip}$ of each group and the value $Y_{ip}$ for each individual are given as the sum weighted by the membership $u_{cip}$.
\begin{equation}
\hat{b}_{ip} = \sum_{c=1}^C u_{cip}b_{ci}^{*} 
\label{eq:weight_gun}
\end{equation}
\begin{equation}
\hat{Y}_{ip} = \sum_{c=1}^C u_{cip}Y_{cip}^{*}
\label{eq:weight_Y}
\end{equation}
Classifying process is as follows. The new individual $p$ is classified into the group $i$ whose $\hat{b}_{ip}$ is in the proximity to $\hat{Y}_{ip}$. 

%%%%%%%%%%%%%%%%%%%%（data format for the prediction）%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htb]
	\caption{Format of new data for prediction of group} 
	\label{tab:pre_data}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}  \hline
			\multicolumn{1}{|c|}{individual}     & \multicolumn{3}{c|}{item 1}                                                        & \multicolumn{3}{c|}{\ldots}                         & \multicolumn{3}{c|}{item $R$}                                                        \\\cline{2-10}
			\multicolumn{1}{|c|}{}      & \multicolumn{3}{c|}{\hspace{1mm}1 \hspace{3mm}\ldots \hspace{3mm}$x_1$}  & \multicolumn{3}{c|}{\ldots}                        & \multicolumn{3}{c|}{\hspace{1mm}1  \hspace{3mm}\ldots  \hspace{3mm}$x_R$}   \\\hline

{1}       & \multicolumn{3}{c|}{\hspace{-15mm}$\surd$}                                        & \multicolumn{3}{c|}{}                               & \multicolumn{3}{c|}{\hspace{-15mm}$\surd$}     \\

{\vdots} & \multicolumn{3}{c|}{\hspace{-1mm}}                                         & \multicolumn{3}{c|}{\vdots}                              & \multicolumn{3}{c|}{\hspace{-15mm}}    \\

{$N$}  & \multicolumn{3}{c|}{\hspace{15mm}$\surd$}                                                 & \multicolumn{3}{c|}{ }          & \multicolumn{3}{c|}{\hspace{15mm}$\surd$}    \\
\hline  
		\end{tabular}
	\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
It should be noted that the membership of an individual to the fuzzy cluster can be calculated only when the group, to which the individual is belonging to, is known. 
When we are given the additional new data as in Table \ref{tab:pre_data} and our task is to predict the group to which each of the individuals belongs, we have no idea to estimate the memberships to fuzzy clusters. Contrary to what was indicated for initially given data, we need to know the group to calculate the memberships. This point seems to be a major disadvantage against the conventional method in which no memberships are needed. Our sophisticated measure is to assume that each newly given individual belongs to every group one by one and then judge the group to which the individual is most likely to belong by comparing the values of $\hat{b}_{ip}$ and $\hat{Y}_{ip}$.  


The membership of the data $l$ is estimated as in Eqs.(\ref{eq:u})-(\ref{eq:ei}).
\begin{eqnarray}
	u_{cl} = \frac{\exp (A_{cl})}{\displaystyle{\sum_{c^{'}=1}^C} \exp( A_{c^{'}l})}  
        \label{eq:pre_u}
\end{eqnarray}

{\small
\begin{eqnarray}
 A_{c^{'}l} = \frac{1}{\tau_0}(\bar{Y}_{c^{'}i} - \bar{Y}_{c^{'}})^2 - \frac{\lambda_{c^{'}}}{\tau_0} (Y_{c^{'}l} - \bar{Y}_{c^{'}})^2 \label{eq:pre_ei}
\end{eqnarray}
}
where $Y_{cl}$ is as in Eq.(\ref{eq:ex_quan})
\begin{equation}
Y_{cl} = \sum_{j=1}^{R}\sum_{k=1}^{x_j}(a_{cjk}+\overline{Item}_{cj})\delta_{l}(jk) 
\end{equation}
Since the group $i$ is unknown, by assuming that the individual belongs to one of each group, $u_{cl}$ is estimated and then by using this value, like Eqs.(\ref{eq:weight_gun})-(\ref{eq:weight_Y}), 
\begin{equation}
\hat{b}_{l} = \sum_{c=1}^C u_{cl}b_{ci}^{*} 
\label{eq:pre_weight_gun}
\end{equation}
\begin{equation}
\hat{Y}_{l} = \sum_{c=1}^C u_{cl}Y_{cl}^{*}
\label{eq:pre_weight_Y}
\end{equation}
are obtained. From the respective values of $\hat{b}_{l}$ and $\hat{Y}_{l}$, we can judge the group to which the individual is most likely to belong.  The detail procedure of the judgment is exemplified by using the numerical example in the next section.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Numerical Example
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Numerical Example}

A Questionnaire used in market research for opening a new restaurant is shown in Table \ref{tab:Question} ~\cite{questionnaire}. The purposes of the data analysis are as follows:
\begin{enumerate}
\item Estimate the ratio of latent customers who are not sure to become a customer of the restaurant.
\item Review the profiles of prospective customers and conduct an in-depth analysis for increasing the number of customers.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%% Questionnaire sheet %%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[thb]
\caption{Questionnaire sheet}
\vspace{3mm}
\label{tab:Question}
\fbox{
\begin{tabular}{c}
Our new restaurant ABC will start to service\\
 on November 1. Taking our menus into account, \\
please answer each questions below.\\
\\
\hspace{-12mm}Question 1 Do you have an intention to be\\
 a customer of restaurant ABC?\\
\fbox{
\begin{tabular}{c}
1. Yes\hspace{7mm}2. No\hspace{7mm}3. Not decided yet\\
\end{tabular}
}\\
\\
\hspace{-12mm}Question 2 What lunch do you typically take?\\
  Please select up to 5 dishes.\\
\fbox{
\begin{tabular}{c}
\hspace{-2mm}1. Cutlet bowl\hspace{3mm}2. Tempura bowl\hspace{3mm}3. Eel bowl\\
\hspace{-3mm}4. Beaf bowl\hspace{3mm}5. China bowl\hspace{3mm}6. Sashimi lunch\\
\hspace{-5mm}7. Broiled fish lunch\hspace{5mm}8. Oden lunch\\
\hspace{-5mm}9. Yakitori lunch \hspace{5mm}10. Pork lunch\\
\hspace{-5mm}11.  Hamburger lunch\hspace{5mm}12. Larmen lunch\\
\hspace{-5mm}13. Soba lunch\hspace{5mm}14. Rice and curry\\
\hspace{-5mm}15. Sushi
\end{tabular}
}\\
\\
\hspace{-15mm}Question 3 Which one do you think \\
should come along with your meal?\\
\fbox{
\begin{tabular}{c}
\hspace{30mm}prefer\hspace{4mm}neutral \hspace{4mm}dislike \\
\hspace{-3mm}A. Liquor \ldots \hspace{3mm}1\hspace{12mm}2\hspace{12mm}3\\
\hspace{-3mm}B. Sweets \ldots \hspace{3mm}1\hspace{12mm}2\hspace{12mm}3\\
\hspace{-3mm}C. Coffee \ldots \hspace{3mm}1\hspace{12mm}2\hspace{12mm}3
\end{tabular}
}\\
\\
\hspace{-13mm}Question 4 Gender \\
\fbox{
\begin{tabular}{c}
1. Male \hspace{23mm}2. Female \hspace{15mm}
\end{tabular}
}\\
\\
\hspace{-13mm}Question 5 Age\\
\fbox{
\begin{tabular}{c}

\end{tabular}
}\hspace{2mm}years
\end{tabular}
}

\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The summary of the responses about the intention of becoming a customer is as follows:
\begin{enumerate}
\item Yes:15
%\vspace{-3mm}
\item No:10
%\vspace{-3mm}
\item Not decided yet:25
\end{enumerate}

\hspace{-3.6mm} Since half of the responders are not decided yet, it is most important to solicit and lure this group of customers. For the analysis of frequency count data from independent items, Cramer's V is used to examine the association or correlation between them~\cite{questionnaire}. The 15 dishes in the menu and their frequency count of binary outcome are used to examine the correlation with the intension of becoming a customer (Yes or No). We cross-tabulated the data to form 15 2-by-2 tables. The chi-square test statistic is a comparison of observed frequencies ($O_j$) to expected frequencies ($E_j$), where the
observed frequencies come from the data and the expected frequencies are hypothetical occurrence of what one would encounter if the null hypothesis were true.
Expected frequencies are calculated as the product of the values in the margins of the tables divided by the total sample size for both items. 
Pearson's ("uncorrected") chi-square test statistic is:
\begin{equation}
\chi^2 = \sum_{j} \frac{(O_j-E_j)^2}{E_j}
\label{eq:pearson}
\end{equation}
%
where $O_j$ is the observed frequency in table cell $j$ and $E_j$ is the expected frequency in table cell $j$. The test statistic has $(r - 1)(c - 1)$ degree of freedom, where $r$ represents the number of rows in the frequency table and $c$ represents its number of columns. Cramer's V is calculated by correcting the chi squared statistic for sample and table size: 
\begin{equation}
V = \sqrt{\frac{\chi ^2}{n(k-1)}}
\label{eq:cramer}
\end{equation}
%
where $n$ is the total number (frequency) and $k$ is either $r$ or $c$ (whichever is smaller). On the basis of the Cramer's V, we selected 6 items from those in Question 2, that affect the customers' intention. The 15 responders who answered affirmatively and 10 negatively are classified by our proposed method for testing the classification performance. All responders are correctly classified. By the conventional quantifying method, the classification ratio is 80 percent. The nonlinearity due to the membership function seems to have contributed to this improvement. The weighting coefficient for fuzzification in clustering provides the gradation of nonlinearity. The totally fuzzy clusters ($\tau_0 = \infty$) derive the same results as one by the conventional method, since all individuals have the same memberships.\\
Now we illustrate how to classify the newly given individual whose membership in a group is not known, hence the membership in a cluster is also unknown. We predicted how many of them among 25 (50 percent) responders, who did not give definite answer(Yes or No), have the similar characteristics as those who are willing to become a customer.
A part of the prediction results is given in Table \ref{tab:potential}.

%%%%%%%%%%%%%%%%%%%（Classifying expected customers）upper %%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table}[htbp]
\caption{Prediction of latent customers}
\label{tab:potential}
\begin{center}
\begin{tabular}{|c|c|cccccc|c|} \hline
Jud & Ind & \multicolumn{6}{c|}{Distance from group mean}  & Gr \\\cline{3-8}
ge & No. & \multicolumn{3}{c}{2} & \multicolumn{3}{c|}{1} & No. \\\hline

1 & 1 &  & $\mid$ &  & 0.53 & $\mid$ & & 1 \\
  &   &  & $\mid$ &  &  & $\mid$ & 0.03 & 1 \\\hline

2 & 2 &  & $\mid$ & 0.72 & & $\mid$ &  & 2 \\
  &   &  & $\mid$ & 0.72 & & $\mid$ &  & 2 \\\hline

2 & 3 & 0.55 & $\mid$ &  &  & $\mid$ & & 2 \\
  &   & 0.55 & $\mid$ &  &  & $\mid$ & & 2 \\\hline

1 & 10&  & $\mid$ & & 0.45 & $\mid$ &  & 1 \\
  &   &  & $\mid$ & 0.82 &  & $\mid$ & & 2 \\\hline

2 & 11& 0.46 & $\mid$ &  &  & $\mid$ & & 2 \\
  &   & 0.46 & $\mid$ &  &  & $\mid$ & & 2 \\\hline

1 & 12&  & $\mid$ &  &  & $\mid$ & 0.79 & 1 \\
  &   &  & $\mid$ &  &  & $\mid$ & 0.79 & 1 \\\hline

\end{tabular}
\end{center}
\end{table}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Table \ref{tab:potential} shows the way to classify group unknown individuals. The left most column indicates the final judgment of group. Individual's number is shown in Ind No. column. The real number shows the absolute difference from the group mean whose value $\hat{b}_{l}$ is in the proximity to the value $\hat{Y}_{l}$. The number 2 and 1 indicate the position of group means. For example, Individual 1 is classified as Group 1(Expected customer) when the individual is assumed to be in Group 1 (as shown in the upper row) and also when assumed to be in Group 2 (as shown in the lower row), the rational judgment is apparently Group 1 since $\hat{Y}_{l}$ appears in the right side of the Group 1 mean which is denoted by 1. So the final judgment is Group 1. As to individual 10, in both cases (upper and lower row) the real values are written between the two group means and the judgments are different from each other. In this ambiguous case, the smaller distance is chosen. So the final judgment is Group 1. Most of the other individuals are apparently classified. Among the 25 responders, 17 of them have the similar characteristics with the positive responders and they can be considered to be the expected customers. The ratio of latent customers is 64 percent.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
We have proposed a nonlinear version of the classical discriminant analysis technique known as the second Hayashi's quantification method for categorical data. Although the notion of membership and FCM clustering algorithm are introduced, the aim of the proposed algorithm is not only the analysis of locally partitioned data but to make the underlying discriminant model nonlinear. The fuzzy clusters are implicitly used for the analysis and the classification of new data.


\begin{thebibliography}{99}

\bibitem{Hayashi}
	C. Hayashi, 
	On the prediction of phenomena from qualitative data and the 
	quantification of qualitative data from the mathematical statistical point of view, 
	Ann. the Institute of Statistical Math.,
	Vol.3, pp.69-98, 1952.

\bibitem{Bezdek}
	J. C. Bezdek, 
	Pattern Recognition with Fuzzy Objective Function Algorithms, 	Plenum Press, New York, 1981.

\bibitem{Miyamoto}
	S. Miyamoto and M. Mukaidono, 
	Fuzzy $c$-means as a regularization and maximum entropy approach,
	Proc. IFSA'97, Vol.II, pp.86-92, 1997.

\bibitem{questionnaire}
	T. Kan, 
	Analysis of Questionnaire, Gendaisugakusha,1998(in Japanese).

\end{thebibliography}
\end{document}


