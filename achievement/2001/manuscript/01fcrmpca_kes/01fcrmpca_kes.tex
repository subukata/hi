\documentclass{article}
\usepackage{graphicx}


\addtolength{\oddsidemargin}{-25mm}  %
\addtolength{\evensidemargin}{-25mm}  %
\setlength{\textheight}{240mm}    %
\setlength{\textwidth}{170mm}     %
\setlength{\topmargin}{0mm}       %
\setlength{\headsep}{0mm}         %


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% New Command and Environment
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% boldmath define
\newcommand{\bmath}[1]{\mbox{\boldmath $#1$}}
\newcommand{\vctr}[1]{\mbox{\boldmath $#1$}}

\pagestyle{plain}



\begin{document}

\title{Knowledge Discovery from Local Principal Components Independent of Arbitrary Factors
\thanks{Knowledge-Based Intelligent Information Engineering Systems \& Allied Technologies, ed. by N. Baba, L. C. Jain and R. J. Howlett, IOS Press, 17-21 (2001)}
}

\author{Chi-Hyon Oh, Katsuhiro Honda, and Hidetomo Ichihashi\\
 Graduate School of Engineering, Osaka Prefecture University,\\
 Gakuen-cho 1-1, Sakai, Osaka, 599-8531 Japan\\ 
}

\date{}

\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Abstract                  %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
In this paper, we propose a technique of extracting local principal components independent of arbitrary factors chosen. The proposed method takes advantage of Fuzzy $c$-Regression \mbox{Models} (FCRM) to estimate the parameters of regression models for fuzzy clusters. We decompose the fuzzy scatter matrix of each cluster into two matrices by using the partial regression coefficient matrix obtained by the FCRM. One is closely related to the arbitrary factors and the other is independent of them. Solving the eigen-value problem of the decomposed matrix enables us to extract the local principal components in which influences of arbitrary factors are neutralized. We apply our method to a POS transaction data set in order to discover useful knowledge from it.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%                Introduction              %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction} 

Principal component analysis (PCA) is a technique to compress multidimensional variables into a few indices. Through the observation of those indices, correlations among variables are extracted and we are able to discover some knowledge from the data set. Though we can gain general knowledge from the data set by the PCA, sometimes it could be trivial or valueless because the PCA extract the indices, {\it i.e.} principal components, so as to have them include the dominant feature of the data set. It might be necessary to analyze data sets from various points of view if one wants to accumulate several characteristics of them. Yanai \cite{Yanai} proposed a technique which has capability of neutralizing influences of arbitrary factors chosen. In \cite{Yanai}, Yanai extracted principal components independent of the factors by exploiting the regression analysis technique. 

Nonetheless, it is not sufficient to only get rid of influences of arbitrary factors considering lots of real data sets have some substructures. Respective examination of each substructure could lead to appropriate analyses of data sets. Fuzzy clustering is an effective vehicle to partition data sets into some substructures and has a lot of varieties. Fuzzy $c$-Regression Models (FCRM) \cite{Hathaway} proposed by Hathaway {\it et al.} is one of those algorithms in which parameters of regression models for clusters, {\it i.e.} partial regression coefficients, are estimated. 

In this paper, we propose a technique of extracting local principal components independent of arbitrary factors. Firstly, we implement the FCRM to derive the parameters of $c$-regression models in our method. The fuzzy scatter matrix is decomposed into two matrices then in the same manner as Yanai's approach. We can obtain local principal components independent of arbitrary factors by solving eigen-value problem of the decomposed fuzzy scatter matrix. We apply our method to a POS transaction data set in which each data point is composed of 20 attributes such as meteorological element and sales of two different supermarkets to gain diverse knowledge from it. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Extraction of Local Principal Components Independent of Arbitrary Factors %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Extraction of Local Principal Components Independent of Arbitrary \mbox{Factors}}

Yanai's approach \cite{Yanai} to extract principal components independent of arbitrary factors takes advantage of the regression analysis technique. It decomposes the variance covariance matrix derived in principal component analysis into two parts by using the obtained partial regression coefficient. One is closely related to external criteria and the other is independent of them. In Yanai's approach, the factors one intends to neutralize are regarded as the external criteria. Taking substructures of data sets into consideration, the partial regression coefficient for each of them is estimated in our method. We employ Hathaway's FCRM \cite{Hathaway} for the purpose.

In Yanai's approach, it is assumed that the mean value of the data set is zero. We, therefore, introduce the concept of cluster center to the original FCRM. Let $Y_c$ and $X_c$ be sets of response variables, {\it i.e.} external criteria, and explanatory variables for each cluster $c$. They can be written as follows:
%
\begin{eqnarray}
Y_c = ({\vctr y}^T_{c1}, {\vctr y}^T_{c2}, \cdots, {\vctr y}^T_{cn})^T = \{ y_{cki} \},  \qquad
y_{cki} = y_{ki} - v_{ci}^y, \ \ i = 1, \cdots, t,
\end{eqnarray}
%
\begin{eqnarray}
X_c = ({\vctr x}_{c1}^T, {\vctr x}_{c2}^T, \cdots, {\vctr x}_{cn}^T)^T = \{ x_{ckj} \},  \qquad
x_{ckj} = x_{kj} - v_{cj}^x, \ \ j = 1, \cdots, s,
\end{eqnarray}
%
where $T$ denotes transposition. $t$ and $s$ are the dimensionalities of response and explanatory variables and $n$ is the number of data points. Note that ${\vctr y}^T_{ck}$ and ${\vctr x}_{ck}^T$ are represented row vectors for convenience sake of notation of the following equations. $v_{ci}^y$ and $v_{cj}^x$ are cluster centers of response and explanatory variables.

Suppose we identify the following $C$ linear models to extract local linear structures.
%
\begin{eqnarray}
{\vctr y}_c = {\vctr x}_c B_c + {\vctr e}_c, \hspace{3mm} c=1, \cdots, C,
\end{eqnarray}
%
where $B_c$ is the partial regression coefficient matrix for each cluster $c$ and can be written as follows: 
%
\[B_c = \left(
\begin{array}{@{\,}cccc@{\,}}
b_{c11} & b_{c12} & \ldots & b_{c1t} \\
b_{c21} & b_{c22} & \ldots & b_{c2t} \\
\vdots & \vdots & \ddots & \vdots \\
b_{cs1} & b_{cs2} & \ldots & b_{cst} \\
\end{array}
\right) \]
%
$B_c$ is calculated by least squares method in the general linear regression analysis. 

The FCRM can be driven by optimization of an objective function. We use Lagrange's method of indeterminate multiplier to derive the objective function for the FCRM. We have the following objective function to be minimized.
%
\begin{eqnarray}
L &=& \sum_{c=1}^C \sum_{k=1}^n u_{ck} \| {\vctr y}_{ck} -  {\vctr x}_{ck}B_c \|^2 
 + T_0 \sum_{c=1}^C \sum_{k=1}^n u_{ck} \log u_{ck}
+ \sum_{k=1}^n T_k \Big( \sum_{c=1}^C u_{ck} - 1 \Big). 
\label{eq:object}
\end{eqnarray}
%
$u_{ck}$ is membership of the data point $k$ to the cluster $c$ which represents degree of belonging to clusters. $u_{ck}$ satisfies the following condition.
%
\begin{eqnarray}
{\vctr u}_c \in \{ (u_{ck}) | \sum_{c=1}^C u_{ck}= 1, u_{ck} \in [ 0,1 ], 
c = 1, \cdots , C \}.
\label{eq:mem}
\end{eqnarray}
%
In (\ref{eq:object}), the first term represents residual sum of squares. The second term represents entropy maximization as regularization which was introduced in Fuzzy $c$-Means by Miyamoto {\it et al.} \cite{Miyamoto} for the first time. It enables us to obtain fuzzy clusters. $T_0$ is the weighting parameter which specifies degree of fuzziness. The remaining terms describe the constraint of the membership $u_{ck}$, {\it i.e.} (\ref{eq:mem}). $T_k$ is the Lagrangian multiplier.

From the necessary condition $ {\partial L}/{\partial b_{cji}} = 0$ for the optimality of the objective function $L$, $B_c$ can be derived as follows:
%
\begin{eqnarray}
B_c = (X_c^T D_c X_c)^{-1} X_c^T D_c Y_c.
\label{eq:B_c}
\end{eqnarray}
%
In (\ref{eq:B_c}), $D_c$ is the diagonal matrix, diagonal entries of which are memberships of data points.
%
\[D_c = \left(
\begin{array}{cccc}
u_{c1} & 0 & \ldots & 0 \\
0 & u_{c2} & \ldots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \ldots & u_{cn} \\
\end{array}
\right) \]
%
From the necessary conditions ${\partial L}/{\partial u_{ck}} = 0$, ${\partial L}/{\partial v_{cj}^x} = 0$ and ${\partial L}/{\partial v_{ci}^y} = 0$ for $L$, we also have the following equations for the membership $u_{ck}$, the cluster center of the explanatory variable $v_{cj}^x$ and the cluster center of the response variable $v_{cj}^y$.
%
\begin{eqnarray}
	u_{ck} = \exp (A_{ck}) / \displaystyle{\sum_{a=1}^c \exp( A_{ak})}, \ \  
	A_{ak} = - \frac{1}{T_0}\sum_{i=1}^t \Big( y_{aki} - \sum_{j=1}^s x_{akj}b_{aij} \Big)^2, \end{eqnarray}
%
\begin{eqnarray}
v_{cj}^x = \displaystyle{\sum_{k=1}^n u_{ck} x_{kj}} / \displaystyle{\sum_{k=1}^n u_{ck}},
\end{eqnarray}
%
\begin{eqnarray}
v_{cj}^y = \displaystyle{\sum_{k=1}^n u_{ck} y_{kj}} / \displaystyle{\sum_{k=1}^n u_{ck}}.
\end{eqnarray}
%
The solution algorithms for the FCRM are based on an iterative procedure. We remark them below.
\\ \\
\noindent
{\it FCRM Algorithms}

\noindent
Step 1: Set the number of clusters $C$, the coefficient of the entropy term $T_0$ and the\\ 
\hspace*{13mm} terminal condition $\epsilon$. Initialize the membership $u_ck$ randomly so as to have \\
\hspace*{13mm} it follow the condition in (12).\\
Step 2: Update the cluster center $v_{cj}^x$ and $v_{cj}^y$ using (10) and (11).\\
Step 3: Calculate the partial regression coefficient matrix $B_c$ using (7).\\
Step 4: Update the membership $u_{ck}$ using (9).\\
Step 5: If $\max | u_{ck}^{NEW} - u_{ck}^{OLD} | < \epsilon $, then stop. Otherwise, return to Step 2.
\\

We can represent the set of response variables $Y_c$ as in (\ref{eq:decomp_yc}) by using the partial regression coefficient matrix $B_c$.
%
\begin{eqnarray}
Y_c &=& X_c B_c + \{ Y_c - X_c B_c \} \nonumber \\
&=& Y_{Xc} + Y_{Xc}^{-}.
\label{eq:decomp_yc}
\end{eqnarray}
%
In (\ref{eq:decomp_yc}), $Y_{Xc}$ is the predicted value by $X_c$. On the one hand, $Y_{Xc}^{-}$ is the unaccountable part according to $X_c$. It is, therefore, able to be said that $Y_{Xc}^{-}$ is independent of $X_c$. We can divide fuzzy scatter matrix $S_{fc}$ into two parts in the same manner as (\ref{eq:decomp_yc}).
%
\begin{eqnarray}
S_{fc} &=& Y_c^T D_c Y_c \nonumber \\ 
&=& Y_c^T D_c (Y_{X_c}+Y_{X_c}^{-}) \nonumber \\
&=& Y_{c}^T D_c X_c B_c + \{Y_c^T D_c Y_c -Y_c^T D_c X_c B_c \} \nonumber \\
&=& S_{fc}^X + S_{fc}^{X-},
\label{eq:decomp_sfc}
\end{eqnarray}
%
wehre $S_{fc}^X$ is the factor matrix accountable by $X_c$ and $S_{fc}^{X-}$ is independent of $X_c$. Using (\ref{eq:B_c}), we can denote $S_{fc}^X$ and $S_{fc}^{X-}$ as follows: 
%
\begin{eqnarray}
S_{fc}^X &=&Y_{c}^T D_c X_c (X_c^T D_c X_c)^{-1} X_c^T D_c Y_c,
\end{eqnarray}
%
\begin{eqnarray}
S_{fc}^{X-} &=& Y_c^T D_c Y_c - Y_c^T D_c X_c (X_c^T D_c X_c)^{-1} X_c^T D_c Y_c.
\end{eqnarray}
%
These factor matrices are both symmetric. We can extract principal components independent of explanatory variables by solving the eigen-value problem of the factor matrix $S_{fc}^{X-}$. We can neutralize the influence of arbitrary factors by setting them as explanatory variables.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Knowledge Discovery from a POS Transaction Data Set %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Knowledge Discovery from a POS Transaction Data Set}
%
We applied the proposed method to a POS (Point of Sales) transaction data set of two different supermarkets to discover useful knowledge from it. The data set consists of 333 data, which have 20 items, the number of customers having come to the store, meteorological element and so on. We enumerate each item and item number below. 
\\ \\
\noindent
{\it Items of the POS transaction data set}
\begin{quotation}
\noindent
1: Holiday, 2: Friday, 3: Saturday, 4: Sunday, 5: Average temperature of the day, 6, 7, 8 and 9: Temperature at 6, 12, 15 and 18 o'clock, 10: Humidity, 11 and 12: Weather category during day and night, 13: Precipitation, 14, 15 and 16: Precipitation during 9-12, 12-15 and 15-18 o'clock, 17 and 18: the number of customers of supermarket A and B, 19 and 20: Sales of perishables of supermarket A and B\end{quotation}

\noindent
The items of days of week, Holyday, Friday, Saturday, Sunday, are dummy variables and weather categories are represented by integer values. 

We applied Fuzzy $c$-Varieties (FCV) \cite{Bezdek}, which is one of the fuzzy clustering algorithm proposed by Bezdek {\it et al.}, to the POS data set before applying our method to it. In the FCV, prototypes of clusters are multi dimensional linear varieties. Since the linear varieties are spanned by some local principal component vectors, the FCV can be regarded as a simultaneous algorithm of fuzzy clustering and the PCA. Through the analysis of the obtained local principal components, we can discover some knowledge of each cluster. We set the number of clusters to two. The data set was divided into two seasonal clusters, the hot and cold seasons. After analyzing the local principal components obtained by the FCV, we gain the knowledge which says precipitation and the sales of perishables have a close correlation. The less it rains, the more the sales increase.

We applied our method to the POS transaction data set on the basis of the results of the FCV. Since we have obtained the relation with precipitation and the sales of perishables, we chose precipitation as the factor to be neutralized its influence. We show the value of fuzzy factor loading \cite{Yabuuchi} of each item in Table \ref{tab:ff_load}. The fuzzy factor loading quantifies correlations between local principal components and each item. When a certain item has the same sign of the fuzzy factor loading as the other items, it has positive correlation with them. Therefore, if the value of an item increases, that of the other item which has the same sign of the fuzzy factor loading also increases and vice versa. We also specified the number of clusters as two in our method. The data set was divided into the hot and cold seasons. In Table \ref{tab:ff_load}, we underlined noteworthy values. Observing the values of fuzzy factor loading, we can mention that the numbers of customers of both supermarkets increase on Saturday and Sunday not Friday and the sales grow in both seasons since Friday has the value of fuzzy factor loading which has the opposite sign of the sales of perishables. Furthermore, when the temperature comes down, the sales go up. In this manner, we can gain diverse knowledge from the data set by using our proposed method.

\begin{table*}[bt]
\caption{Fuzzy Factor Loading.}
%
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|} \hline
Item \#& 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 & 10 \\\hline\hline
Cold season & 0.01 & \underline{-0.48} & \underline{0.35} & \underline{0.39} & -0.12 & -0.12 & -0.09 & -0.06 & -0.10 & -0.28 \\\hline
Hot season & -0.01 & \underline{-0.47} & \underline{0.30} & \underline{0.53} & \underline{-0.46} & \underline{-0.45} & \underline{-0.43} & \underline{-0.46} & \underline{-0.46} & -0.11 \\\hline
\end{tabular}
%
\end{center}
\begin{center}
\begin{tabular}{|c||c|c|c|c|c|c|c|c|c|c|} \hline
Item \# & 11 & 12 & 14 & 15 & 16 & 17 & 18 & 19 & 20 \\\hline\hline
Cold season & -0.33 & -0.32 & -0.30 & -0.22 & -0.19 & \underline{0.78} & \underline{0.87} & \underline{0.92} & \underline{0.88} \\\hline
Hot season & -0.02 & 0.11 & -0.14 & -0.15 & -0.02 & \underline{0.75} & \underline{0.86} & \underline{0.90} & \underline{0.92} \\\hline
\end{tabular}
\end{center}
%
\label{tab:ff_load}
\end{table*}

%%%%%%%%%%%%%%
% Conclusion %
%%%%%%%%%%%%%%

\section{Conclusion}
In this paper, we proposed a technique of extracting local principal components independent of arbitrary factors. Firstly, we estimate the partial regression coefficient of each cluster by using the FCRM and decompose it into two matrices then. We can extract the local principal components in which influences of arbitrary factors are neutralized from the decomposed matrix. In the numerical example, we applied the proposed method to a POS transaction data set. Our method has the capability of discovering diverse knowledge from the data set.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%         Bibliography       %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{thebibliography}{99}

\bibitem{Yanai}
H. Yanai, Factor Analysis with External Criteria, Japanese Psychological Research 12, 4 (1970) 143-153.

\bibitem{Hathaway}
R. J. Hathaway and J. C. Bezdek, Switching Regression Models and Fuzzy Clustering, IEEE Trans. on Fuzzy Systems 1, 3 (1993) 195-204.

\bibitem{Miyamoto}
S. Miyamoto and M. Mukaidono, Fuzzy $c$-Means as a Regularization and Maximum Entropy Approach, Proc. IFSA'97 2 (1997) 86-92.

\bibitem{Bezdek}
J.C.Bezdek, C.Coray, R.Gunderson, and J.Watson, Detection and Characterization of Cluster Substructure 2. Fuzzy $c$-Varieties and Convex Combinations Thereof, SIAM J. Appl. Math. 40, 2 (1981) 358-372.

\bibitem{Yabuuchi}
Y.Yabuuchi and J.Watada, Fuzzy Principal Compoment Analysis and Its Application, Biomedical Fuzzy and Human Sciences 3, 1 (1997) 83-92.

\end{thebibliography}

\end{document}
